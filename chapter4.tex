\section{Future Work}
\subsection{Bayesian Non-parametric Learning for the SSSM}\label{sec:non-parameteric}
\subsubsection{Introduction to Hierarchical Dirichlet Process}
The Dirichlet process (DP)~\citep{ferguson1973bayesian} defines a distribution over probability measures on a parameter space $\Theta$. It is parameterized by $G_0$, a base distribution, and $alpha$, a concentration parameter. The DP consists of discrete atoms that are distributed on the base measure $G_0$ with mass that depends on $\alpha$.

\begin{equation}\label{eq:dirichlet_process}
  \begin{split}
    G \mid G_0, \alpha &= \sum\limits_{k=1}^{\infty} \beta_k \delta_{\theta_k} \\
    \theta_k \sim G_0
  \end{split}
\end{equation}

$\delta$ in Equation~\ref{eq:dirichlet_process}, refers to the Dirac delta function and represents the atoms of $G$ that are distributed according to $\theta_k \sim G_0$. The atoms have mass $\beta_k$, where $\beta \sim GEM(\alpha)$~\citep{neal2000markov}. Note that the Dirichlet process consists of infinitely many atoms; the term Bayesian non-parametrics stems from this countably infinite number of parameters. Because the draws from a DP are discrete, there is a positive probability that multiple observations $\theta_k \sim G_0$ take identical values. This is known as the clustering property of the DP.

The Chinese Restaurant Process (CRP)~\citep{neal2000markov, gershman2012tutorial}, is an abstraction of the DP. It marginalizes over the probability associated with an atom and instead presents the DP in terms of the clusters that are formed. The CRP imagines a restaurant with an infinite number of tables, tables being the components in the mixture model. Customers (data) arrive at the restaurant and choose a table proportional to the number of customers already at that table ($n_k$), or choose a new table with probability proportional to $\alpha$. As more customers enter the restaurant, more tables are chosen with:
\begin{equation}
  E[N_t] = \alpha \log(N_c)
\end{equation}
$N_t$ and $N_c$ refer to the number of tables and customers respectively~\citep{gershman2012tutorial}. The DP can be interpreted as an infinite mixture model by assigning $\theta_k$ to denote some cluster specific parameters. The number of components is random and grows as new data are observed. It is important to understand that while conceptually, and for the generative model, there are an infinite number of components, in practice a finite dataset exhibits a finite number of clusters (only a finite number of tables can have customers seated at them)~\citep{blei2006variational}.

The hierarchical Dirichlet process (HDP) extends Equation~\ref{eq:dirichlet_process} by placing a Dirichlet process prior on many group specific Dirichlet processes~\citep{teh2005sharing}. The associated abstraction is the Chinese Restaurant Franchise (CRF) where restaurants may have menus that offer the same dish, but may also have dishes that are restaurant specific. This allows restaurants to assign different mass to table clusters where the customers still have the same dish. Now customers choose a restaurant in a franchise and choose a dish from the restaurant of choice. The HDP draws $G_0$ from a Dirichlet process $DP(\gamma, H)$, and it draws group (restaurant) specific distributions $G_j \sim DP(\alpha, G_0)$. The base measure $G_0$ acts as the expected value for encoding the frequency of each global, shared parameter~\citep{fox2007hierarchical}.

\begin{equation}
  E[G_j \mid G_0] = G_0
\end{equation}

\subsubsection{Hierarchical Dirichlet Process for Hidden Markov Models}
We may wish to use the HDP as the clustering prior to infer the parameters and transition probabilities in a HMM. We assume an unknown number of regimes and thus model this with a DP prior. Simply using a DP prior is insufficient for modeling HMM dynamics as the DP would place a static probability on observing the next state $X_t \mid X_{t-1}$ for all possible $X_{t-1}$ which is clearly not the case for the HMM. The transition to state $X_t$ from $X_{t-1}$ must depend on state specific probabilities $\pi_{X_{t-1}}$ and not some global partition prior $\pi$. The HMM therefore involves a set of mixture models that each depend on a specific state. The state indexes a row of the transition matrix, where the probabilities in this row correspond to the mixing proportions for the choice of the next state. We therefore encode this state (regime) dependent transition by using the HDP which still encourages structure in the individual transitions. Now each regime $m$ might have its specific transition probabilities $\pi_m$ but the different regimes might share the affinity to transition to certain `dominant' regimes.

\cite{fox2009nonparametric, fox2007hierarchical} discuss problems with the HDP approach. The HDM-HMM inadequately models the temporal persistence of states. Each state is allowed to have a unique transition mixture, with mass shared among states for certain transitions that are more probable. However, it is impossible to encourage self transitions with simply the base hierarchical parameter $H$. The result is that the HDP-HMM exhibits a rapid inferred switching from one state to the next~\citep{fox2007hierarchical}. Rather, if we introduce a higher probability of a self transition, we encourage the HMM to have an affinity for remaining in any given regime for a greater length of time. This directly ties to Assumption 2 in Section~\ref{sec:inference_for_sssm}.

The adjustment to the HDP-HMM that \cite{fox2009nonparametric,fox2007hierarchical} makes is to add a self-transition affinity parameter $kappa$. The resulting model is termed the sticky hierarchical Dirichlet process for hidden Markov models (sticky HDP HMM). Inference is performed using a modified Gibbs sampler for the HDP~\citep{teh2005sharing}. This model presents an attractive alternative to Algorithm~\ref{alg:constrained_alg} as the number of regimes is not pre-defined and more importantly, the model specification allows the growth of the number of regimes with the length and/or complexity of the data. This captures the intuitive reality of the simulation more accurately.

An avenue for the extension of the \cite{fox2009nonparametric} model is to encourage the linear growth of the number of regimes with the length of a given session. The Pitman-Yor process~\citep{pitman1997two} extends the DP for linear growth of clusters (not logarithmic as presented above). \cite{blunsom2011hierarchical} has applied this concept to develop the hierarchical Pitman-Yor process HMM. It seems natural to extend these models to the \cite{fox2007hierarchical,fox2009nonparametric} sticky-HDP HMM model.

\subsection{The SSSM as an Assistive Classroom Tool}\label{sec:class-assistive}
With the introduction of rich and complex learning environments, we should remain aware that these simulations pose challenges for teachers who wish to structure learning around the simulation's outcome. These teachers may require additional tools to assist them with the domain specific challenges that arise when designing lesson plans around these outcomes. In Section~\ref{sec:need_for_assistive_tech}, we presented how it is hard to track the state of the simulation and to identify the salient learning opportunities that arise from the students' interactions with the simulation. Moreover, standard evaluation metrics might not be available for activities engendered by these complex environments. The investigation of how to integrate these tools into the class is therefore of paramount importance.

Section~\ref{sec:user_evaluation} presents a study that demonstrates how SSSM is used to decompose a large session from CW into small periods that individually are interpretable. This thesis has focused mainly on the time series data itself and has presented techniques for modeling the effects of students' actions on the system state. Future work will investigate the application of these models for producing an assistive system for implementation in the classroom. Thus the investigation into the techniques for presenting a wholistic picture of the session that is meaningful to the students has been left for future studies.

We propose two principles for choosing information that is relevant to present to students and teachers:
\begin{itemize}
  \item \textbf{Personal salience}: include scenarios from the simulation experience that are likely to be memorable for the students.
  \item \textbf{Explanatory coherence}: include a subset of the simulation’s causal chains that enables students’ discussion of an aspect of the underlying explanatory model.
\end{itemize}

The work here has focused solely on the \textit{explanatory coherence} topics. A full assistive model will not only include information that describes system dynamics and changes to the system state, but it will also highlight key elements from the simulation that will be important to the individual students. The work involves designing, implementing and testing this classroom tool.

Another exciting avenue for future research involves exploring the trade-off that is made between the predictive power of a model and the explanatory coherence that the model achieves. \cite{wu2017beyond} have suggested a method for regularizing deep learning models to facilitate people's understanding of their predictions. This is an important balance to review and one that we intend to consider in educational settings.

\section{Conclusion}

This thesis has made three contributions. Firstly, we have recognized that a complex exploratory learning environment might require assistive tools to help teachers and students review meaningful information from a given session. This novel research has studied the possibilities for extracting such information from the log files of an exploratory learning environment. We used the Connected World simulation as a test case and represented the logs as a time series. It was our hypothesis that a time series can be decomposed into shorter periods that are individually more interpretable and manageable than the time series as a whole.

Secondly, we have applied SSSMs to the task of decomposing the time series into shorter and individually coherent periods. Our work has built upon previous time series analysis tools and has presented a novel algorithm for learning the change points and regime parameters that are associated with the SSSM. We have further conducted a survey of possible future work in Bayesian non-parameterics for allowing the data to influence the number of regimes that are inferred in a given session.

Lastly, we have designed two user based experiments that test the understandibility and change point relevance of the model output. The human interpretability of the model was the ultimate goal of this investigation. However, evaluating the different aspects of the model (`learning' vs `quantification') was a challenging task. Our experiments suggest that the model not only finds a good representation of the periods that might be present in Connected Worlds, but that it can also summarize each period (of about 30 seconds) into two to three lines of brief text. The generated text captures much of the dynamics of an associated video representation of the period. Between the two studies, we show that it is possible to simplify a complex time series into periods of activity that are human interpretable.

We have left the design and implementation of a classroom tool as future work. This tool should aim to interactively support teachers and students for post session reviews. This work presents exciting new possibilities for classroom artificial intelligence tools that can support learning in these rich and immersive exploration environments.
