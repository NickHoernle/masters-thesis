\section{Future Work}
\subsection{Bayesian Non-parametric Learning for the SSSM}\label{sec:non-parameteric}
The Dirichlet process (DP)~\cite{ferguson1973bayesian} is a probability measure on probability measures. It is parameterized by $G_0$, a base distribution, and $alpha$, a concentration parameter and consists of discrete atoms that are distributed on the base measure $G_0$ with mass that depends on $\alpha$.

\begin{equation}\label{eq:dirichlet_process}
  \begin{split}
    G \mid G_0, \alpha &= \sum\limits_{k=1}^{\infty} \beta_k \delta_{\theta_k} \\
    \theta_k \sim G_0
  \end{split}
\end{equation}

In Equation~\ref{eq:dirichlet_process}, $\delta$ refers to the derac delta function and represents the atoms of $G$ that are distributed according to $\theta_k ~ G_0$ and have mass $\beta_k$ associated with them. Note that in the Dirichlet process formalism, the number of atoms is assumed to be $\infty$, and thus the term Bayesian non-parametrics stems from this countably infinite number of parameters. The Chinese Restaurant Process (CRP)~\citep{neal2000markov, gershman2012tutorial}, is an abstraction that marginalizes over the probability associated with an atom and instead presents the Dirichlet process in terms of the clusters that are formed. The CRP imagines a restaurant with an infinite number of tables (components in the mixture model). Customers (data) arrive at the restaurant and choose a table proportional to the number of clients already at that table $n_k$, or choose a new table with probability proportional to $\alpha$. As more customers enter the resturant, more tables are chosen with $E[N_t] = \alpha \log(N_c)$ ($N_t$ and $N_c$ refer to the number of tables and customers respectively). The DP can be used as a mixture model by assuming $\theta_k$ denotes some cluster specific paramters and where the number of components is random and grows as new data are observed. It is important to understand that while concepturally there are an infinite number of components, in practice a finite dataset exxhibits a finite number of clusters (only a finite number of tables can have customers seated at them).

The hierarchical Dirichlet process (HDP) extends Equation~\ref{eq:dirichlet_process} by placing a Dirichlet process prior on the Dirichlet process~\citep{teh2005sharing}. The associated abstraction is the Chinese Restaurant Franchise (CRF) where restaurants may have menus that offer the same dish, but may also have dishes that are restaurant specific. This allows restaurants to assign different mass to table clusters where the customers still have the same dish. The HDP draws $G_0$ from a Dirichlet process $DP(\gamma, H)$, and then it draws group (restaurant) specific distributions $G_j \sim DP(\alpha, G_0)$. The base measure $G_0$ now acts as the expected value for encoding the frequency of each, global shared parameter ($E[G_j \mid G_0] = G_0$).

We may wish to use the HDP as the clustering prior to infer the HMM clustering parameters. We assume an unknown number of regimes and thus model this with a DP prior. Simply using a DP prior is insufficient for modeling HMM dynamics as the DP would place a static probability on observing the next state $X_t \mid X_{t-1}$ which is clearly not the case for the HMM. The transition to state $X_t$ from $X_{t-1}$ must depend on state dependent probabilities $\pi_X$ and not some global partition prior $\pi$. We therefore encode this regime dependent transition by using the HDP which still encourages structure in the individual transitions. Now each regime $j$ might have its specific transition probabilities $\pi_j$ but the different regimes might share the affinity to transition to certain `dominant' regimes. \cite{fox2009nonparametric, fox2007hierarchical} demonstrate the problems with this approach. While we have allowed a reasonable clustering property to find the regimes in the HMM, and each regime is allowed to have its regime specific transition probabilities, it is impossible to encourage self transitions with simply the base hierarchical parameter $H$. The self transition states that the HMM should have an affinity for remaining in any given regime, when it is in that regime. This directly ties to Assumption 2 in Section~\ref{sec:inference_for_sssm}.

\cite{fox2009nonparametric, fox2007hierarchical} present an adjustment to the HDP, by adding a self-transition affinity parameter $K$. The resulting model is termed the sticky hierarchical Dirichlet process for hidden Markov models (sticky HDP HMM). Inference is performed using a modified Gibbs sampler for the HDP~\cite{teh2005sharing}. This model presents an attractive alternative to Algorithm~\ref{alg:constrained_alg} as the number of regimes is not pre-defined and is allowed to grow with the length and/or complexity of the data. An avenue for the extension of the \citep{fox2009nonparametric} model is to encourage the linear growth of the number of regimes with the length of a given session. The Pitman-Yor process~\citep{pitman1997two} extends the DP for linear growth of cluters. \cite{teh2006hierarchical} has applied this model to the HDP setting. It seems natural to extend these models to \cite{fox2009nonparametric} sticky-HDP HMM model.

\subsection{The SSSM as an Assistive Classroom Tool}\label{sec:class-assistive}
With the introduction of rich and complex learning environments, we should remain aware that these tools may require additional tools to assist teachers when integrating the technology implementation into the classroom. Moreover, the domains of these assistive tools can imply that standard evaluation metrics are not necessarily available. The integration of these tools into the classroom 

A final avenue for future research involves exploring the trade-off that is made between the predictive power of a model and the explanatory coherence that the model achieves.  Wu et al.~\cite{wu2017beyond} have suggested a method for regularizing deep learning models to facilitate people's understanding of their predictions. This is an important balance to consider and one that we intend to consider in educational settings.

\section{Conclusion}

% This paper has presented novel research into the simplification of log files that are generated by complex participatory immersive simulations. The log files were represented as a time series that was decomposed with the long term goal of producing periods that are useful for a teacher when leading reflective discussions about students' sessions. We have built upon previous time series analysis tools to formulate a model that automatically segments a time series into these salient periods. We find that evaluators are independently able to validate the inferred changes between the automatically generated periods. This preliminary study demonstrates that it is possible to simplify a complex time series into periods of activity that are human interpretable.  Our focus now rests on designing assistive tools for teachers that can facilitate their understanding of students' interactions in multi-participant immersive simulations.
