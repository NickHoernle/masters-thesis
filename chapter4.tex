\section{Future Work}
\subsection{Bayesian Non-parametric Learning for the SSSM}\label{sec:non-parameteric}
The Dirichlet process (DP)~\cite{ferguson1973bayesian} defines a distribution over probability measures on a parameter space $\Theta$. It is parameterized by $G_0$, a base distribution, and $alpha$, a concentration parameter. The DP consists of discrete atoms that are distributed on the base measure $G_0$ with mass that depends on $\alpha$.

\begin{equation}\label{eq:dirichlet_process}
  \begin{split}
    G \mid G_0, \alpha &= \sum\limits_{k=1}^{\infty} \beta_k \delta_{\theta_k} \\
    \theta_k \sim G_0
  \end{split}
\end{equation}

In Equation~\ref{eq:dirichlet_process}, $\delta$ refers to the Dirac delta function and represents the atoms of $G$ that are distributed according to $\theta_k ~ G_0$. The atoms have mass $\beta_k$, where $\beta ~ GEM(\alpha)$~\citep{neal2000markov}. Note that the Dirichlet process consists of infinitely many atoms, and thus the term Bayesian non-parametrics stems from this countably infinite number of parameters. Because the draws from a DP are discrete, there is a positive probability that multiple observations $\theta_k \sim G_0$ take identical values. This is known as the clustering property of the DP and it makes the DP useful in many clustering applications.

The Chinese Restaurant Process (CRP)~\citep{neal2000markov, gershman2012tutorial}, is an abstraction of the DP. It marginalizes over the probability associated with an atom and instead presents the DP in terms of the clusters that are formed. The CRP imagines a restaurant with an infinite number of tables (components in the mixture model). Customers (data) arrive at the restaurant and choose a table proportional to the number of customers already at that table ($n_k$), or choose a new table with probability proportional to $\alpha$. As more customers enter the restaurant, more tables are chosen with $E[N_t] = \alpha \log(N_c)$~\cite{gershman2012tutorial}; $N_t$ and $N_c$ refer to the number of tables and customers respectively. The DP can be interpreted as an infinite mixture model by assigning $\theta_k$ to denote some cluster specific parameter. The number of components is random and grows as new data are observed. It is important to understand that while conceptually there are an infinite number of components, in practice a finite dataset exhibits a finite number of clusters (only a finite number of tables can have customers seated at them)~\citep{blei2006variational}.

The hierarchical Dirichlet process (HDP) extends Equation~\ref{eq:dirichlet_process} by placing a Dirichlet process prior on many group specific Dirichlet processes~\citep{teh2005sharing}. The associated abstraction is the Chinese Restaurant Franchise (CRF) where restaurants may have menus that offer the same dish, but may also have dishes that are restaurant specific. This allows restaurants to assign different mass to table clusters where the customers still have the same dish. Now customers choose a restaurant in a franchise and choose a dish from the restaurant of choice. The HDP draws $G_0$ from a Dirichlet process $DP(\gamma, H)$, and it draws group (restaurant) specific distributions $G_j \sim DP(\alpha, G_0)$. The base measure $G_0$ now acts as the expected value for encoding the frequency of each global, shared parameter ($E[G_j \mid G_0] = G_0$) \citep{fox2007hierarchical}.

We may wish to use the HDP as the clustering prior to infer the parameters and transition probabilities in a HMM. We assume an unknown number of regimes and thus model this with a DP prior. Simply using a DP prior is insufficient for modeling HMM dynamics as the DP would place a static probability on observing the next state $X_t \mid X_{t-1} \forall X_{t-1}$ which is clearly not the case for the HMM. The transition to state $X_t$ from $X_{t-1}$ must depend on state specific probabilities $\pi_X$ and not some global partition prior $\pi$. The HMM therefore involves a set of mixture models that each depend on a specific state. The state indexes a row of the transition matrix, where the probabilities in this row correspond to the mixing proportions for the choice of the next state. We therefore encode this state (regime) dependent transition by using the HDP which still encourages structure in the individual transitions. Now each regime $m$ might have its specific transition probabilities $\pi_m$ but the different regimes might share the affinity to transition to certain `dominant' regimes.

\cite{fox2009nonparametric, fox2007hierarchical} discuss the problems with the HDP approach. The HDM-HMM inadequately models the temporal persistence of states. Each state is allowed to have a unique transition mixture, with mass shared among states for certain transitions that are more probable. However, it is impossible to encourage self transitions with simply the base hierarchical parameter $H$. The result is that the HDP-HMM exhibits a rapid inferred switching from one state to the next~\citep{fox2007hierarchical}. Rather, if we introduce a higher probability of a self transition, we encourage the HMM to have an affinity for remaining in any given regime for a greater length of time. This directly ties to Assumption 2 in Section~\ref{sec:inference_for_sssm}.

The adjustment to the HDP-HMM that \cite{fox2009nonparametric} and \cite{fox2007hierarchical} make is to add a self-transition affinity parameter $kappa$. The resulting model is termed the sticky hierarchical Dirichlet process for hidden Markov models (sticky HDP HMM). Inference is performed using a modified Gibbs sampler for the HDP~\cite{teh2005sharing}. This model presents an attractive alternative to Algorithm~\ref{alg:constrained_alg} as the number of regimes is not pre-defined and more importantly, the model specification allows the growth of the number of regimes with the length and/or complexity of the data. This captures the intuitive reality of the simulation more accurately.

An avenue for the extension of the \cite{fox2009nonparametric} model is to encourage the linear growth of the number of regimes with the length of a given session. The Pitman-Yor process~\citep{pitman1997two} extends the DP for linear growth of clusters (not logarithmic as presented above). \cite{blunsom2011hierarchical} has applied this concept to develop the hierarchical Pitman-Yor process HMM. It seems natural to extend these models to the \cite{fox2009nonparametric} sticky-HDP HMM model.

\subsection{The SSSM as an Assistive Classroom Tool}\label{sec:class-assistive}
With the introduction of rich and complex learning environments, we should remain aware that these simulations may require additional tools to assist teachers when integrating the technology implementation into classroom learning. Moreover, standard evaluation metrics might not be available for the learning opportunities encountered by these tools. The investigation of how to integrate these tools into the class is therefore of paramount importance.

Section~\ref{sec:user_evaluation} presents a study that demonstrates the SSSM can be used to decompose a large session from CW into small periods that individually are interpretable. Future work will investigate the application of these models for producing an assistive system that can actually be implemented in the classroom. This thesis has focused mainly on the time series data itself and has presented techniques for modeling the effects of students' actions on the system state. An avenue for further work is to investigate techniques for presenting a wholistic picture of the session that is meaningful to the students.

We propose two principles for choosing information that is relevant to present to students and teachers:
\begin{itemize}
  \item Personal salience: include scenarios from the simulation experience that are likely to be memorable for the students.
  \item Explanatory coherence: include a subset of the simulation’s causal chains that enables students’ discussion of an aspect of the underlying explanatory model.
\end{itemize}

The work here has focused solely on the \textit{explanatory coherence} topics. A full assistive model will not only look to include information that describes system dynamics and changes to the system state, but it will also highlight key elements from the simulation that will be important to the individual students. Designing, implementing and testing this classroom tool is left as future work.

Another exciting avenue for future research involves exploring the trade-off that is made between the predictive power of a model and the explanatory coherence that the model achieves. \cite{wu2017beyond} have suggested a method for regularizing deep learning models to facilitate people's understanding of their predictions. This is an important balance to review and one that we intend to consider in educational settings.

\section{Conclusion}

This thesis has made three contributions. Firstly, we have recognized that a complex exploratory learning environment might require assistive tools to help teachers and students review meaningful information from a given session. This paper has presented novel research into the possibilities for extracting such information from the log files of an exploratory learning environment. We used the Connected World simulation as a test case and represented the logs as a time series. It was out hypothesis that a time series can be decomposed into shorter periods that individually might be more interpretable and manageable than the time series as a whole.

Secondly, we have applied switching state space models to the task of decomposing complex the time series into shorter and individually coherent periods. Our work has built upon previous time series analysis tools and has presented a novel algorithm for learning the change points and regime parameters that are associated with the SSSM. We have further conducted a survey of possible future work in Bayesian non-parameterics for allowing the data to influence the number of regimes that are inferred in a given session.

Lastly, we have designed two user based experiments that test the understandibility and change point relevance of the model output. The human interpretability of the model was the ultimate goal of this investigation. However, evaluating the different aspects of the model (`learning' vs `quantification') was a challenging task. Our experiments suggest that the model not only finds a good representation of the periods that might be present in Connected Worlds, but that it can also summarize each period (of about 30 seconds) into two to three lines of brief text. The generated text captures much of the dynamics of an associated video representation of the period. Between the two studies, we show that it is possible to simplify a complex time series into periods of activity that are human interpretable.

We have left the design and implementation of a classroom tool as future work. This tool should aim to interactively support teachers and students for post session reviews. This work presents exciting new possibilities for classroom artificial intelligence tools that can support learning in these rich and immersive exploration environments.
