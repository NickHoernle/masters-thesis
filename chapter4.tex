\section{Future Work}
There are two areas to be considered for future work. Firstly, Algorithm~\ref{alg:constrained_alg} assumes a known number of regimes. This is problematic in that sessions of different lengths or complexities would naturally have a different number of periods. Choosing this number in advance is challenging yet, as was seen in Section~\ref{sec:experiment2-empirical-validation} it can have a major effect on the algorithm output. Bayesian non-parametrics presents a technique for remaining agnostic about the number of regimes to search for. In Section~\ref{sec:non-parameteric}, review related work in Bayesian non-parametrics for HMMs and we motivate why these models are promising for this domain. In Section~\ref{sec:class-assistive} we discuss the future work that builds upon the work in this thesis to work towards our original goal of presenting a tool that assists teachers when reviewing sessions from Connected Worlds.

\subsection{Bayesian Non-parametric Learning for the SSSM}\label{sec:non-parameteric}
\subsubsection{Introduction to Hierarchical Dirichlet Process}
The Dirichlet process (DP)~\citep{ferguson1973bayesian} defines a distribution over probability measures on a parameter space $\Theta$. It is parameterized by $G_0$, a base distribution, and $\alpha$, a concentration parameter. The DP consists of discrete atoms that are distributed on the base measure $G_0$ with mass that depends on $\alpha$.

\begin{equation}\label{eq:dirichlet_process}
  \begin{split}
    G \mid G_0, \alpha &= \sum\limits_{k=1}^{\infty} \beta_k \delta_{\theta_k} \\
    \theta_k \sim G_0
  \end{split}
\end{equation}

$\delta$ in equation~\ref{eq:dirichlet_process}, refers to the Dirac delta function and represents the atoms of $G$ that are distributed according to $\theta_k \sim G_0$. The atoms have mass $\beta_k$, where $\beta \sim GEM(\alpha)$~\citep{neal2000markov}. Note that the Dirichlet process consists of infinitely many atoms; the term Bayesian non-parametrics stems from this countably infinite number of parameters. The DP can be used to model a countably infinite number of mixtures in a mixture model. Here each $\theta_k$ describes the parameters associated with mixture component $k$ and this component has a mixture weight given by $\beta_k$.

The Chinese Restaurant Process (CRP)~\citep{neal2000markov, gershman2012tutorial}, is an abstraction of the DP. It marginalizes over the probability associated with an atom and instead presents the DP in terms of the clusters that are formed. The CRP imagines a restaurant with an infinite number of tables, tables being the components in the mixture model. Customers (data) arrive at the restaurant and choose a table proportional to the number of customers already at that table, or choose a new table with probability proportional to $\alpha$. As more customers enter the restaurant, more tables are chosen with:
\begin{equation}
  E[N_t] = \alpha \log(N_c)
\end{equation}
$N_t$ and $N_c$ refer to the number of tables and customers respectively~\citep{gershman2012tutorial}. The number of components is random and grows as new data are observed. It is important to understand that while conceptually, and for the generative model, there are an infinite number of components, in practice a finite dataset exhibits a finite number of clusters (only a finite number of tables can have customers seated at them)~\citep{blei2006variational}.

The hierarchical Dirichlet process (HDP) extends equation~\ref{eq:dirichlet_process} by placing a Dirichlet process prior on many group specific Dirichlet processes~\citep{teh2005sharing}. The associated abstraction is the Chinese Restaurant Franchise (CRF) where restaurants may have menus that offer the same dish, but may also have dishes that are restaurant specific. This allows restaurants to assign different mass to table clusters where the customers still have the same dish. Now customers choose a restaurant in a franchise and choose a dish from the restaurant of choice. The HDP draws $G_0$ from a Dirichlet process $DP(\gamma, H)$, and it draws group (restaurant) specific distributions $G_j \sim DP(\alpha, G_0)$. The base measure $G_0$ acts as the expected value for encoding the frequency of each global, shared parameter~\citep{fox2007hierarchical}.

\begin{equation}
  E[G_j \mid G_0] = G_0
\end{equation}

\subsubsection{Hierarchical Dirichlet Process for Hidden Markov Models}
We may wish to use the HDP as the clustering prior to infer the parameters and transition probabilities in a HMM. We assume an unknown number of regimes and thus model this with a DP prior. Simply using a DP prior is insufficient for modeling HMM dynamics as the DP would place a static probability on observing the next state $X_t \mid X_{t-1}$ for all possible $X_{t-1}$ which is clearly not the case for the HMM. The transition to state $X_t$ from $X_{t-1}$ must depend on state specific probabilities $\pi_{X_{t-1}}$ and not some global partition prior $\pi$. The HMM therefore involves a set of mixture models that each depend on a specific state. The state indexes a row of the transition matrix, where the probabilities in this row correspond to the mixing proportions for the choice of the next state. We therefore encode this state (regime) dependent transition by using the HDP which still encourages shared structure between the individual transitions. Now each regime $m$ might have its specific transition probabilities $\pi_m$ but the different regimes might share the affinity to transition to certain `dominant' regimes.

\cite{fox2009nonparametric, fox2007hierarchical} discuss problems with the HDP approach. The HDP-HMM inadequately models the temporal persistence of states. Each state is allowed to have a unique transition mixture, with mass shared among states for certain transitions that are more probable. However, it is impossible to encourage self transitions with simply the base hierarchical parameter $H$. The result is that the HDP-HMM exhibits a rapid inferred switching from one state to the next~\citep{fox2007hierarchical}. Rather, if we introduce a higher probability of a self transition, we encourage the HMM to have an affinity for remaining in any given regime for a greater length of time. This directly ties to Assumption 2 in Section~\ref{sec:inference_for_sssm}.

The adjustment to the HDP-HMM that \cite{fox2009nonparametric,fox2007hierarchical} propose is to add a self-transition affinity parameter $\kappa$. The resulting model is termed the sticky hierarchical Dirichlet process for hidden Markov models (sticky HDP HMM). Inference is performed using a modified Gibbs sampler for the HDP~\citep{teh2005sharing}. This model presents an attractive alternative to Algorithm~\ref{alg:constrained_alg} as the number of regimes is not pre-defined and more importantly, the model specification allows the growth of the number of regimes with the length and/or complexity of the data. This captures the intuitive reality of the simulation more accurately in that we would want more regimes to describe longer and more complex sessions.

An avenue for the extension of the \cite{fox2009nonparametric} model is to encourage the linear growth of the number of regimes with the length of a given session. The Pitman-Yor process~\citep{pitman1997two} extends the DP for linear growth of clusters (not logarithmic as presented above). \cite{blunsom2011hierarchical} have applied this concept to develop the hierarchical Pitman-Yor process HMM. It seems natural to extend these models to include the regime affinity parameter $\kappa$ from \cite{fox2007hierarchical,fox2009nonparametric}.

\subsection{The SSSM as an Assistive Classroom Tool}\label{sec:class-assistive}
With the introduction of rich and complex learning environments, we should remain aware that these simulations pose challenges for teachers who wish to structure learning around a class's session. These teachers may require additional tools to assist them with the domain specific challenges that arise when designing lesson plans around these sessions. In Section~\ref{sec:need_for_assistive_tech}, we discussed how it is hard for a participant or observer to track the state of the CW simulation. It is therefore also challenging to identify the salient learning opportunities that arise from the students' interactions with the simulation. In addition to this, standard evaluation metrics might not be available for activities engendered by these complex simulations. The investigation of how to integrate these tools into the class is therefore of paramount importance.

Section~\ref{sec:user_evaluation} presents a study that demonstrates how the SSSM is used to decompose a large session from CW into small periods that individually are interpretable. This thesis has focused mainly on the time series data itself and has presented techniques for modeling the effects of students' actions on the system state. Future work will investigate the application of these models for producing an assistive system for implementation in the classroom. An investigation into the techniques for presenting a meaningful, holistic picture of the session remains for future studies.

The design of classroom assistive tools should focus on what information they present to the students as well as how they present the information. We propose two principles for choosing information that is relevant to present to students and teachers:
\begin{itemize}
  \item \textbf{Personal salience}: includes scenarios from the simulation experience that are likely to be memorable for the students.
  \item \textbf{Explanatory coherence}: includes a subset of the simulation’s causal chains that enables students’ discussion of an aspect of the underlying explanatory model.
\end{itemize}

The work in this thesis has focused solely on the \textit{explanatory coherence} topics. A full assistive model will not only include information that describes system dynamics and changes to the system state, but it will also highlight key elements from the simulation that will be important to the individual students. The proposed future work involves designing, implementing and testing this classroom tool.

Another exciting avenue for future research involves exploring the trade-off that is made between the predictive power of a model and the explanatory coherence that the model achieves. \cite{wu2017beyond} have suggested a method for regularizing deep learning models to facilitate people's understanding of their predictions. This is an important balance to review and one that we intend to consider in educational settings.

\section{Conclusion}

This thesis has made three contributions. Firstly, we have recognized that complex exploratory learning environments might require assistive tools to help teachers and students review meaningful information from a given interaction session. This novel research has studied the possibilities for extracting information from the log files of an exploratory learning environment. We used the Connected World simulation as a test case and represented the logs as a time series. It was our hypothesis that a time series can be decomposed into shorter periods that are individually more interpretable and manageable than the session as a whole.

Secondly, we have applied switching state space models to the task of decomposing the time series into shorter and individually coherent periods. Our work has built upon previous time series analysis tools and presents an algorithm for learning the change points and regime parameters that are associated with the switching state space model. We have further conducted a survey of possible future work in Bayesian non-parameterics for allowing the data to influence the number of regimes that are inferred in a given session. A flexible number of regimes is especially appealing for a setting such as Connected Worlds where the sessions vary dramatically in length and complexity.

Lastly, we have designed two user-based experiments that test the understandibility and change point relevance of the model output. The human interpretability of the model was the ultimate goal of this investigation. However, evaluating the different aspects of the model (`learning' vs `quantification') was a challenging task. Our experiments suggest that the model not only finds a good representation of the periods that might be present in Connected Worlds, but that it also summarizes each period (of about 30 seconds) into a brief two to three lines of text. The generated text captures much of the dynamics of an associated video representation of the period. Between the two studies, we show that it is possible to simplify a complex time series into periods of activity that are human interpretable.

We have left the design and implementation of a classroom tool as future work. This tool should aim to interactively support teachers and students for post session reviews. This work presents exciting new possibilities for classroom artificial intelligence tools that can support learning in these rich and immersive exploration environments.
