\section{Future Work}
\subsection{Bayesian Non-parametric Learning for the SSSM}\label{sec:non-parameteric}
The Dirichlet process (DP)~\cite{ferguson1973bayesian} is a probability measure on probability measures. It is parameterized by $G_0$, a base distribution, and $alpha$, a concentration parameter and consists of discrete atoms that are distributed on the base measure $G_0$ with mass that depends on $\alpha$.

\begin{equation}\label{eq:dirichlet_process}
  \begin{split}
    G \mid G_0, \alpha &= \sum\limits_{k=1}^{\infty} \beta_k \delta_{\theta_k} \\
    \theta_k \sim G_0
  \end{split}
\end{equation}

In Equation~\ref{eq:dirichlet_process}, $\delta$ refers to the Dirac delta function and represents the atoms of $G$ that are distributed according to $\theta_k ~ G_0$ and have mass $\beta_k$ associated with them. Note that in the Dirichlet process consists of infinitely many atoms, and thus the term Bayesian non-parametrics stems from this countably infinite number of parameters.

The Chinese Restaurant Process (CRP)~\citep{neal2000markov, gershman2012tutorial}, is an abstraction that marginalizes over the probability associated with an atom and instead presents the Dirichlet process in terms of the clusters that are formed. The CRP imagines a restaurant with an infinite number of tables (components in the mixture model). Customers (data) arrive at the restaurant and choose a table proportional to the number of clients already at that table $n_k$, or choose a new table with probability proportional to $\alpha$. As more customers enter the restaurant, more tables are chosen with $E[N_t] = \alpha \log(N_c)$ ($N_t$ and $N_c$ refer to the number of tables and customers respectively). The DP can be used as a mixture model by assuming $\theta_k$ denotes some cluster specific parameters and where the number of components is random and grows as new data are observed. It is important to understand that while conceptually there are an infinite number of components, in practice a finite dataset exhibits a finite number of clusters (only a finite number of tables can have customers seated at them).

The hierarchical Dirichlet process (HDP) extends Equation~\ref{eq:dirichlet_process} by placing a Dirichlet process prior on the Dirichlet process~\citep{teh2005sharing}. The associated abstraction is the Chinese Restaurant Franchise (CRF) where restaurants may have menus that offer the same dish, but may also have dishes that are restaurant specific. This allows restaurants to assign different mass to table clusters where the customers still have the same dish. The HDP draws $G_0$ from a Dirichlet process $DP(\gamma, H)$, and it draws group (restaurant) specific distributions $G_j \sim DP(\alpha, G_0)$. The base measure $G_0$ now acts as the expected value for encoding the frequency of each global, shared parameter ($E[G_j \mid G_0] = G_0$).

We may wish to use the HDP as the clustering prior to infer the HMM clustering parameters. We assume an unknown number of regimes and thus model this with a DP prior. Simply using a DP prior is insufficient for modeling HMM dynamics as the DP would place a static probability on observing the next state $X_t \mid X_{t-1}$ which is clearly not the case for the HMM. The transition to state $X_t$ from $X_{t-1}$ must depend on state dependent probabilities $\pi_X$ and not some global partition prior $\pi$. The HMM involves a set of mixture models that depends on the current state. The current state therefore indexes a specific row of the transition matrix, where the probabilities in this row correspond to the mixing proportions for the choice of the next state. We therefore encode this regime dependent transition by using the HDP which still encourages structure in the individual transitions. Now each regime $j$ might have its specific transition probabilities $\pi_j$ but the different regimes might share the affinity to transition to certain `dominant' regimes.

\cite{fox2009nonparametric, fox2007hierarchical} discuss the problems with the HDP approach. The HDM-HMM inadequately models the temporal persistence of states. Each state is allowed to have a unique transition mixture, with mass shared among states for certain transitions that are more probable. However, it is impossible to encourage self transitions with simply the base hierarchical parameter $H$. The result is that the HDP-HMM exhibits a rapid inferred switching from one state to the next~\citep{fox2007hierarchical}. Rather, if we introduce a higher probability of a self transition, we encourage the HMM to have an affinity for remaining in any given regime. This directly ties to Assumption 2 in Section~\ref{sec:inference_for_sssm}.

% state persistence

\cite{fox2009nonparametric, fox2007hierarchical} present an adjustment to the HDP-HMM, by adding a self-transition affinity parameter $K$. The resulting model is termed the sticky hierarchical Dirichlet process for hidden Markov models (sticky HDP HMM). Inference is performed using a modified Gibbs sampler for the HDP~\cite{teh2005sharing}. This model presents an attractive alternative to Algorithm~\ref{alg:constrained_alg} as the number of regimes is not pre-defined and is allowed to grow with the length and/or complexity of the data.

An avenue for the extension of the \citep{fox2009nonparametric} model is to encourage the linear growth of the number of regimes with the length of a given session. The Pitman-Yor process~\citep{pitman1997two} extends the DP for linear growth of cluters. \cite{teh2006hierarchical} has applied this model to the HDP setting. It seems natural to extend these models to \cite{fox2009nonparametric} sticky-HDP HMM model.

\subsection{The SSSM as an Assistive Classroom Tool}\label{sec:class-assistive}
With the introduction of rich and complex learning environments, we should remain aware that these tools may require additional tools to assist teachers when integrating the technology implementation into the classroom. Moreover, the domains of these assistive tools can imply that standard evaluation metrics are not necessarily available. The investigation of how to integrate these tools into the class is therefore of paramount importance.

Section~\ref{sec:user_evaluation} presents a study that demonstrates the interpretability of the SSSM for decomposing a large session from CW into small periods that individually are interpretable. Future work will investigate the application of these models for producing an assistive system that can actually be implemented in the classroom. We have focused heavily on the time series data and modeling the effects of students' actions on the system state. An avenue for further work is to integrate other available data to present a wholistic picture of the session that is meaningful to the students.

We propose two principles for choosing information that is relevant to present to students and teachers:
\begin{itemize}
  \item Personal salience: include scenarios from the simulation experience that are likely to be memorable for the students.
  \item Explanatory coherence: include a subset of the simulation’s causal chains that enables students’ discussion of an aspect of the underlying explanatory model.
\end{itemize}

The work that is presented in this thesis has focused solely on the \textit{explanatory coherence} topics. A full assistive model will not only look to include information that desctibes system dynamics and changes to the system state, but it will also highlight key elements from the simulation that will be important to the indiviudal students. Designing, implementing and testing this classroom tool is left as future work.

A final avenue for future research involves exploring the trade-off that is made between the predictive power of a model and the explanatory coherence that the model achieves. \cite{wu2017beyond} have suggested a method for regularizing deep learning models to facilitate people's understanding of their predictions. This is an important balance to consider and one that we intend to consider in educational settings.

\section{Conclusion}

This paper has presented novel research into the simplification of log files that are generated by complex participatory immersive simulations. The log files were represented as a time series that was decomposed with the long term goal of producing periods that are useful for a teacher when leading reflective discussions about students' sessions. We have built upon previous time series analysis tools to formulate a model that automatically segments a time series into these salient periods. We find that evaluators are independently able to validate the inferred changes between the automatically generated periods. This preliminary study demonstrates that it is possible to simplify a complex time series into periods of activity that are human interpretable.  Our focus now rests on designing assistive tools for teachers that can facilitate their understanding of students' interactions in multi-participant immersive simulations.
