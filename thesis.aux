\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{Abstract}{ii}}
\@writefile{toc}{\contentsline {chapter}{Introduction}{1}}
\newlabel{ch:intro}{{}{2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Connected Worlds}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:1}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Connected Worlds}{3}}
\newlabel{sec:connected_worlds}{{1.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Introduction}{3}}
\newlabel{sec:connected_worlds_intro}{{1.1.1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Bird's eye view of the CW simulation. Biomes are labeled on the perimeter and logs appear as thick red lines. Water enters via the waterfall and in this image it is mainly flowing from left to right toward the Desert and the Plains.\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:connected_worlds_graphic}{{1.1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Water Cycle}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Plant-Animal Relations}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Flow chart showing the water cycles that are present in CW. Blue solid lines represent main water flows, dotted lines represent other possible flows of water that are often not present or are negligable. Green, diamond boxes represent the actions that the students can take that will change the water cycles that are present. Grey circles are areas of the simulation and grey boxes are specifically one of the four biomes.\relax }}{6}}
\newlabel{fig:system_overview_water}{{1.2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Need for Assistive Technology}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Map of the relationships between plants, animals and biomes in the CW environment. The biomes are shown in yellow. The blue boxes correspond to areas of the simulation that do support animals but do not have plants that grow there. The fauna-flora specific relations can clearly be seen by the edges that link the plants (green ovals) and animals (red ovals). The plants and animals are endemic to their biome.\relax }}{8}}
\newlabel{fig:system_overview_plant_animal}{{1.3}{8}}
\citation{ghahramani2000variational}
\citation{fox2009nonparametric}
\citation{fox2007hierarchical,li2003survey}
\citation{fox2009nonparametric,jonsen2007identifying,pavlovic2001learning}
\citation{oh2008learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The Switching State Space Model}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:2}{{2}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Switch Based Models}{9}}
\citation{esling2012time,horst2004data}
\citation{vlachos2003wavelet,tanaka2005discovery,patel2002mining}
\citation{patel2002mining}
\citation{preston2009event}
\citation{ghahramani2000variational}
\citation{pavlovic2001learning}
\citation{giordani2007unified}
\citation{fox2007hierarchical}
\citation{whiteley2010efficient}
\citation{ghahramani2001introduction,shumway2000time}
\citation{ghahramani2000variational}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Background}{11}}
\@writefile{toc}{\contentsline {subsubsection}{State Space Models and Hidden Markov Models}{11}}
\newlabel{sec:state_space_and_hidden_markov_models}{{2.1.1}{11}}
\newlabel{eq:joint_prob_ssm}{{2.1}{11}}
\newlabel{eq:hmm_first_order}{{2.2}{11}}
\citation{shumway2000time}
\@writefile{toc}{\contentsline {subsubsection}{Forward Algorithm}{12}}
\newlabel{eq:fwd_filter}{{2.3}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Backward Algorithm}{13}}
\newlabel{eq:bkwd_filter1}{{2.4}{13}}
\newlabel{eq:bkwd_filter2}{{2.5}{13}}
\citation{forney1973viterbi}
\citation{nasrabadi2007pattern}
\newlabel{eq:bkwd_filter3}{{2.6}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Viterbi Algorithm}{14}}
\newlabel{sec:viterbi_algorithm}{{2.1.1}{14}}
\newlabel{eq:viterbi}{{2.7}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Switching State Space Models}{15}}
\newlabel{sec:switching_state_space_models}{{2.1.2}{15}}
\newlabel{eq:switching_state_space}{{2.8}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Interpretability of the SSSM}{15}}
\newlabel{sec:interpretability_of_ssm}{{2.1.3}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Graphical model for the switching-state space model. A latent discrete switching variable ($S_t$) selects an active, real-valued state space model ($X^{(m)}_t$). The observation vector ($Y_t$) depends on the active regime at time $t$.\relax }}{16}}
\newlabel{fig:switching_ssm}{{2.1}{16}}
\citation{fox2009nonparametric,shumway2000time,kim1994dynamic}
\newlabel{eq:var}{{2.9}{17}}
\newlabel{eq:ssm_rep_of_var_state}{{2.10}{17}}
\newlabel{eq:ssm_rep_of_var_output}{{2.11}{17}}
\citation{murphy2002dynamic,kim1994dynamic}
\citation{murphy2002dynamic,ghahramani2000variational}
\citation{ghahramani2000variational}
\citation{kim1999state,murphy2002dynamic}
\citation{attias2000variational,saul1996exploiting,saul1996mean,blei2003latent}
\citation{fox2009nonparametric,fox2007hierarchical}
\citation{teh2005sharing}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Inference for Switching State Space Models}{18}}
\newlabel{sec:inference_for_sssm}{{2.2}{18}}
\citation{shumway1991dynamic}
\citation{kim1994dynamic,kim1999state}
\citation{bar1993estimation}
\citation{ghahramani2000variational}
\citation{kim1994dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Approximate Maximum Likelihood Estimation for Switching State Space Models}{19}}
\newlabel{sec:gaussian_merging}{{2.2.1}{19}}
\citation{kim1994dynamic}
\newlabel{eq:kim_ssm_expected_val}{{2.12}{20}}
\newlabel{eq:kim_ssm_covariance}{{2.13}{20}}
\newlabel{eq:kim_sssm_expected_val}{{2.14}{20}}
\newlabel{eq:kim_sssm_covariance}{{2.15}{20}}
\citation{kim1994dynamic}
\citation{ghahramani2000variational}
\citation{kim1994dynamic}
\citation{gelman2014bayesian}
\citation{attias2000variational,saul1996exploiting,saul1996mean,blei2003latent}
\citation{turner2011two}
\citation{mackay1998introduction}
\newlabel{eq:collapsed_likelihood}{{2.16}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Background on Inference for Probabilistic Models}{21}}
\@writefile{toc}{\contentsline {subsubsection}{Introduction to Bayesian Inference}{21}}
\newlabel{eq:bayes_rule}{{2.17}{21}}
\citation{mackay1998introduction}
\citation{mackay1998introduction}
\citation{mackay1998introduction}
\citation{mackay1998introduction}
\citation{mackay1998introduction,gelman2014bayesian}
\@writefile{toc}{\contentsline {subsubsection}{Markov Chain Monte Carlo}{23}}
\newlabel{sec:mcmc}{{2.2.2}{23}}
\citation{neal2011mcmc}
\citation{neal2011mcmc}
\citation{hoffman2014no}
\@writefile{toc}{\contentsline {subsubsection}{Hamiltonian Monte Carlo and the No U-Turn Sampler}{24}}
\newlabel{sec:hmc_nuts}{{2.2.2}{24}}
\citation{hoffman2014no}
\citation{carpenter2016stan}
\citation{jasra2005markov}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Algorithm for Posterior Inference of Switching State Space Models}{25}}
\newlabel{sec:posterior_inference_sssm}{{2.2.3}{25}}
\@writefile{toc}{\contentsline {subsubsection}{Nonidentifiability and Label Switching}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Posterior samples and associated density plot from the posterior of a Gaussian mixture model with true parameters $\mu _1 = -1, \mu _2=1, \sigma _1=\sigma _2=0.75$. Left shows the density plot for the posterior of the mean parameters. Right shows the posterior sample trace. The label switches can be seen at samples 1000 and 2000 resulting in the multimodal posterior on the left.\relax }}{26}}
\newlabel{fig:label_switching_example}{{2.2}{26}}
\@writefile{toc}{\contentsline {subsubsection}{Algorithm Sketch}{26}}
\citation{shumway2000time}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Posterior inference algorithm\relax }}{28}}
\newlabel{alg:constrained_alg}{{1}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Updated graphical model showing the semi-supervised switching labels, along with the choice of only two chains between two semi-supervised points. This representation is repeated $M-1$ times to describe the $M-1$ switches between the $M$ regimes. Note that the labeled switch variables at the boundaries ($S_t$ and $S_{t+K}$) are shared between successive regimes.\relax }}{29}}
\newlabel{fig:updated_ssm_graphical_model}{{2.3}{29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}User Study}{30}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:3}{{3}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}User Study Empirical Validation}{30}}
\citation{ghahramani2000variational}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Evaluation on Synthetic Data}{31}}
\newlabel{eq:switching_state_space_results}{{3.1}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example of a generated time series from the SSSM model of Equation~\ref  {eq:switching_state_space_results}. The $x$ axis represents time, and the $y$ axis shows the observations. Regime labels are shown as black and gray dots representing the two label options. True labels (top) are compared to the inferred labels from Algorithm~\ref  {alg:constrained_alg} (middle) and the Gaussian merging (bottom).\relax }}{32}}
\newlabel{fig:result_generated_time_series_with_labels}{{3.1}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Histogram of the percent of correctly inferred labels for the observed output. The structured sampling Algorithm \ref  {alg:constrained_alg} (a) learns the regime labels more accurately than the uninitialized Gaussian merging algorithm (b).\relax }}{32}}
\newlabel{fig:result_generated_histograms}{{3.2}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Validation of Model Interpretability}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Experiment 1}{33}}
\newlabel{sec:experiment1-empirical-validation}{{3.3.1}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Experiment 1 Results}{34}}
\newlabel{sec:experiment1-empirical-validation-results}{{3.3.2}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Expert validation of five different test files from sessions with CW. The bar plot shows the fraction of correctly identified switches between automatically identified periods.\relax }}{35}}
\newlabel{fig:results_expert_validation}{{3.3}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Experiment 2}{35}}
\newlabel{sec:experiment2-empirical-validation}{{3.3.3}{35}}
\newlabel{RF1}{36}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Screenshot of the user interface designed to evaluate the interpretability of Algorithm~\ref  {alg:constrained_alg}. The video representation at (1) is played for the duration of the period. Three plausible descriptions (of which (2) is one of these) are presented and the validator is asked to select the description that best describes the dynamics shown in the video. In this case, \textit  {Desciption 3} is the correct solution.\relax }}{36}}
\newlabel{fig:user_experiment_overview}{{3.4}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Experiment 2 Results}{37}}
\newlabel{sec:experiment2-empirical-validation-results}{{3.3.4}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Bar plot of the results from Experiment 2. There are 5 files that were tested. The fraction of correctly selected period descriptions from Algorithm~\ref  {alg:constrained_alg} is compared to that from the control conditions. The control conditions include uniformly spaced periods for $8$ and $5$ periods respectively.\relax }}{38}}
\newlabel{fig:experiment2_raw_results}{{3.5}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Graphical model for the hierarchical logistic regression that is conducted to determine the intrpretability effect of Algorithm~\ref  {alg:constrained_alg} and Uniform8 over the base Uniform5 condition. The observed data $y$ corresponds to a Bernoulli trial where a description is selected to match a video clip or not. A given trial has a period specific probability of being chosen correctly $p_i$. $p_i$, in turn, depends on the file's ease of labeling and the algorithm effect. The $\alpha _a$ parameters represent the effect of the algorithm on the probability of correctly selecting the appropriate description. The periods depend on the file and algorithm means by a global variance parameter $\sigma $.\relax }}{39}}
\newlabel{fig:hierarchical_logistic_regression}{{3.6}{39}}
\newlabel{eq:generative_sampling_hierarchical_log}{{3.2}{40}}
\citation{ferguson1973bayesian}
\citation{neal2000markov,gershman2012tutorial}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Conclusion}{42}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:4}{{4}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Future Work}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Bayesian Non-parametric Learning for the SSSM}{42}}
\newlabel{sec:non-parameteric}{{4.1.1}{42}}
\newlabel{eq:dirichlet_process}{{4.1}{42}}
\citation{teh2005sharing}
\citation{fox2009nonparametric,fox2007hierarchical}
\citation{fox2009nonparametric,fox2007hierarchical}
\citation{teh2005sharing}
\citation{fox2009nonparametric}
\citation{pitman1997two}
\citation{teh2006hierarchical}
\citation{fox2009nonparametric}
\bibstyle{ecca}
\bibdata{references}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}The SSSM as an Assistive Classroom Tool}{44}}
\newlabel{sec:class-assistive}{{4.1.2}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Conclusion}{44}}
\bibcite{attias2000variational}{{1}{2000}{{Attias}}{{}}}
\bibcite{bar1993estimation}{{2}{1993}{{Bar-Shalom and Li}}{{}}}
\bibcite{blei2003latent}{{3}{2003}{{Blei \textit  {et~al.}}}{{Blei, Ng and Jordan}}}
\bibcite{carpenter2016stan}{{4}{2016}{{Carpenter \textit  {et~al.}}}{{Carpenter, Gelman, Hoffman, Lee, Goodrich, Betancourt, Brubaker, Guo, Li, Riddell \textit  {et~al.}}}}
\bibcite{esling2012time}{{5}{2012}{{Esling and Agon}}{{}}}
\bibcite{ferguson1973bayesian}{{6}{1973}{{Ferguson}}{{}}}
\bibcite{forney1973viterbi}{{7}{1973}{{Forney}}{{}}}
\bibcite{fox2009nonparametric}{{8}{2009}{{Fox \textit  {et~al.}}}{{Fox, Sudderth, Jordan and Willsky}}}
\bibcite{fox2007hierarchical}{{9}{2007}{{Fox \textit  {et~al.}}}{{Fox, Sudderth and Willsky}}}
\bibcite{gelman2014bayesian}{{10}{2014}{{Gelman \textit  {et~al.}}}{{Gelman, Carlin, Stern, Dunson, Vehtari and Rubin}}}
\bibcite{gershman2012tutorial}{{11}{2012}{{Gershman and Blei}}{{}}}
\bibcite{ghahramani2001introduction}{{12}{2001}{{Ghahramani}}{{}}}
\bibcite{ghahramani2000variational}{{13}{2000}{{Ghahramani and Hinton}}{{}}}
\bibcite{giordani2007unified}{{14}{2007}{{Giordani \textit  {et~al.}}}{{Giordani, Kohn and van Dijk}}}
\@writefile{toc}{\contentsline {chapter}{References}{45}}
\bibcite{hoffman2014no}{{15}{2014}{{Hoffman and Gelman}}{{}}}
\bibcite{horst2004data}{{16}{2004}{{Horst and Abraham}}{{}}}
\bibcite{jasra2005markov}{{17}{2005}{{Jasra \textit  {et~al.}}}{{Jasra, Holmes and Stephens}}}
\bibcite{jonsen2007identifying}{{18}{2007}{{Jonsen \textit  {et~al.}}}{{Jonsen, Myers and James}}}
\bibcite{kim1994dynamic}{{19}{1994}{{Kim}}{{}}}
\bibcite{kim1999state}{{20}{1999}{{Kim \textit  {et~al.}}}{{Kim, Nelson \textit  {et~al.}}}}
\bibcite{li2003survey}{{21}{2003}{{Li and Jilkov}}{{}}}
\bibcite{mackay1998introduction}{{22}{1998}{{MacKay}}{{}}}
\bibcite{murphy2002dynamic}{{23}{2002}{{Murphy and Russell}}{{}}}
\bibcite{nasrabadi2007pattern}{{24}{2007}{{Nasrabadi}}{{}}}
\bibcite{neal2000markov}{{25}{2000}{{Neal}}{{}}}
\bibcite{neal2011mcmc}{{26}{2011}{{Neal \textit  {et~al.}}}{{}}}
\bibcite{oh2008learning}{{27}{2008}{{Oh \textit  {et~al.}}}{{Oh, Rehg, Balch and Dellaert}}}
\bibcite{patel2002mining}{{28}{2002}{{Patel \textit  {et~al.}}}{{Patel, Keogh, Lin and Lonardi}}}
\bibcite{pavlovic2001learning}{{29}{2001}{{Pavlovic \textit  {et~al.}}}{{Pavlovic, Rehg and MacCormick}}}
\bibcite{pitman1997two}{{30}{1997}{{Pitman and Yor}}{{}}}
\bibcite{preston2009event}{{31}{2009}{{Preston \textit  {et~al.}}}{{Preston, Protopapas and Brodley}}}
\bibcite{saul1996mean}{{32}{1996}{{Saul \textit  {et~al.}}}{{Saul, Jaakkola and Jordan}}}
\bibcite{saul1996exploiting}{{33}{1996}{{Saul and Jordan}}{{}}}
\bibcite{shumway1991dynamic}{{34}{1991}{{Shumway and Stoffer}}{{}}}
\bibcite{shumway2000time}{{35}{2000}{{Shumway and Stoffer}}{{}}}
\bibcite{tanaka2005discovery}{{36}{2005}{{Tanaka \textit  {et~al.}}}{{Tanaka, Iwamoto and Uehara}}}
\bibcite{teh2006hierarchical}{{37}{2006}{{Teh}}{{}}}
\bibcite{teh2005sharing}{{38}{2005}{{Teh \textit  {et~al.}}}{{Teh, Jordan, Beal and Blei}}}
\bibcite{turner2011two}{{39}{2011}{{Turner and Sahani}}{{}}}
\bibcite{vlachos2003wavelet}{{40}{2003}{{Vlachos \textit  {et~al.}}}{{Vlachos, Lin, Keogh and Gunopulos}}}
\bibcite{whiteley2010efficient}{{41}{2010}{{Whiteley \textit  {et~al.}}}{{Whiteley, Andrieu and Doucet}}}
